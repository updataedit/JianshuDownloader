欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)
[【汇总】2019年4月专题](https://www.jianshu.com/p/e1afed853866)

---
信息量、信息熵、条件熵、信息增益的关系是怎样的？

![](imgs/4324074-657db4d07621ac9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



##信息熵与信息量

在[信息熵和信息量](https://www.jianshu.com/p/2c68ac9a31c4)中我们了解到信息熵表示的是系统的不确定性、随机性，信息熵同样也表现在系统输出的数据统计中，例如扔骰子系统的随机性就表现在一次次点数的记录数据中。

信息量是指某条消息（已知条件）所具有的消除系统不确定的能力。信息量的大小取决于两个方面：系统原有信息熵的大小以及此条消息消除其不确定性的能力。

![](imgs/4324074-6624af4b8fa71090.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



太阳东出西落这个事件非常确定，所以“太阳从东方升起”这个消息的信息量是零；一个十面骰子和一个六面骰子，同样是“投出6点”这个信息，对于前者则是消除了其他9种可能，后者消除5种可能，因此对于十面骰子这个系统来说此信息量更大。

同样“已知投出的点数大于6或小于等于6”这条信息，对于六面骰子系统来说，它的信息量是0，因为这等于没说；但而对于十二面骰子来说它的信息量则是1，因为它可以直接把结果划为均等概率的两种情况之一。

##信息量计算公式

信息量和信息熵是如何计算的？可以参考[信息熵和信息量](https://www.jianshu.com/p/2c68ac9a31c4)的计算公式：

$$H(X)=-\sum _{x \in X }P(x)\log P(x)$$

对于十面骰子来说，它的信息熵是:

$$H(10种均等可能)=-10\times \frac{1}{10}\times log\frac{1}{10}=3.3219$$

而对于五面骰子来说，它的信息熵是：
$$H(5种均等可能)=-log\frac{1}{5}=2.3219$$

这其中的差值就是“十面骰子投出点数大于5或小于等于5”的信息量，同样我们也可以直接计算这条信息的信息量：

$$H(十面骰子大于5或小于等于5两种均等可能)=-log\frac{1}{2}=1$$

如果我们把“已知投出的点数大于4或小于等于4”这个消息作为十面骰子的条件，那么这个条件所带的信息量就是：
$$
\begin{align}
&H(十面骰子大于4或小于等于4两种可能)\\
&=-\frac{4}{10}\times \log\frac{4}{10}-\frac{6}{10}\times \log\frac{6}{10}\\
&=0.52876+0.4422\\
&=0.97096\\
\end{align}
$$

##最大熵

我们看到0.97096小于1，也就是“已知投出的点数大于4或小于等于4”这个消息所携带的信息量要比“已知投出的点数大于5或小于等于5”的所携带的信息量要少。

$$H(10面骰子大于4或小于等于4)<H(10面骰子大于5或小于等于5)$$



**所有可能具有均等概率的情况下，信息熵最大，叫做最大熵**，所以切割成两份的情况下55等分的信息熵最大。

![](imgs/4324074-4b9a2820776b2110.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

更多关于最大熵的内容可以参考[最大熵](https://www.jianshu.com/p/9cf1239fb427)。

##条件熵

条件熵是在某个条件变为确定的情况下系统仍然存在的不确定性即信息熵，这个不确定性H(Y|X)一定小于或等于条件变为已知之前的信息熵，因为它等于原来系统的熵减去被条件所消除的熵：

$$H(Y|X)=HAfter(Y)=HBefore(Y)-HRemove(Y)$$

如果我们把“已知投出的点数大于4或小于等于4”这个消息看做一个条件，那么这个条件下，下一次点数的不确定性即系统的条件熵可以按照这样计算：

$$
\begin{align}
&H(Y|X)\\
&=H(下一次点数|已知大于4或小于等于4两种可能)\\
&=H(下一次点数|已知大于4)+H(下一次点数|已知小于等于4)\\
&=-\frac{4}{10}\times log\frac{1}{4}-\frac{6}{10}\times log\frac{1}{6}\\
&=0.8+1.551\\
&=2.351\\
\end{align}
$$

可以看到条件熵2.351也正好是系统整体信息熵减去条件自身信息熵的结果，即：

$$
\begin{align}
&H(下一次点数|已知大于4或小于等于4两种可能)\\
&= H(下一次点数)-H(大于4或小于等于4两种可能)\\
&= 3.3219-0.97096\\
&= 2.35096\\
\end{align}
$$

也就是有：
$$H(Y|X)=H(Y)-H(X)$$

##信息增益

![](imgs/4324074-4e97f108597592d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


**信息增益，是指这个条件下被消除的不确定性，即在系统信息熵与条件熵的差，系统在条件变为已知情况下前后信息熵的变化值**。

可以参考[信息增益](https://www.jianshu.com/p/322c30ca4abc)的内容来理解。


在这里就是“已知投出的点数大于4或小于等于4”这条消息在十面骰子系统中的信息熵0.97096。

综上，信息增益（IG，Information Gain）可以是系统信息熵减去条件变为已知情况下的系统信息熵：

$$IG(Y|X)=H(Y)-H(Y|X)$$

另外信息增益也可以借助标准的信息熵公式单独求出，如上面直接求H(十面骰子大于4或小于等于4两种可能)=0.97096的过程。

$$IG(Y|X)=H(X)$$


**在指定系统内，如果一个消息条件变为已知，那么它所能消除的不确定性正好等于在此系统它自身中所包含的不确定性，也等于这个条件已知前后系统不确定性的变化量，即信息量等于信息熵等于信息增益**。

同样，对于“已知投出的点数大于6或小于等于6”这条信息在六面骰子系统中，它的信息量是：
$$
\begin{align}
&H(大于6或小于6两种可能)\\
&= -1\times \log1-0\times \log0\\
&= 0+0\\
&= 0\\
\end{align}
$$
在六面系统中这条消息的信息量是0，自身信息熵也是0，对系统的不确定性没有任何帮助。

>注意这里，对于六面骰子，大于6这种条件是不可能发生的，而小于等于6这种条件是必然发生的，所以“已知投出的点数大于6或小于等于6”是等于什么也没说。




---
欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)

---
###每个人的智能新时代
如果您发现文章错误，请不吝留言指正；
如果您觉得有用，请点喜欢；
如果您觉得很有用，欢迎转载~
---
END