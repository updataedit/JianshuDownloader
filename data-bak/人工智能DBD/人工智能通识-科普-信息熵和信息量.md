欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)
[【汇总】2019年4月专题](https://www.jianshu.com/p/e1afed853866)

---

信息论中的熵如何度量的？

![信息论之父克劳德香农Claude Shannon](imgs/4324074-44d64254a09492a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


##信息熵和信息量

信息是否可以有统一的度量标准？
当你收到两条不同信息的时候，是否有方法可以度量那一条包含更多内容？

信息论之父克劳德香农Claude Shannon对这一切给出了数学量化方法，提出信息熵和信息量的概念。

同热力学中熵的概念一致，信息熵也是用于表现系统的无序随机程度。

![](imgs/4324074-f8e00e937a17c308.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


硬币只有正反两面，随机投掷后落地只有两种可能；而骰子有六种可能。所以随意投出的投资比随意投出的硬币具有更多的随机性，或者说硬币的确定性更多一些。

- 信息A：骰子落地显示5点。
- 信息B：硬币落地正面向上。

明显的，信息A的信息量更大，因为它消除了另外5种可能；而信息B则只消除了另外一种可能。

**当一条信息出现的时候，也意味着背后的随机性的消失。信息熵是对系统背后所有随机可能性的度量，信息量是指特定信息能够消除多少随机性（熵）。**

信息熵和信息量之间的关系是什么？

特定信息的出现都是有概率的。比如说“硬币落地正面朝上”这个信息的概率是1/2，而“骰子落地显示5点”这个信息的概率是1/6。

以骰子来看，每个点数的信息都可以消除另外5种随机可能，那么我们把这些信息量相加就得到了所有可以被消除的熵的总和，但需要注意的是，每个点数都只有1/6概率出现，所以我们还需要乘以这个概率，那么我们就得到：

$$H(X)=\sum _{x \in U }P(x)h(x)$$

其中：
- H(X)表示系统X的信息熵；
- U是系统X所有肯能的集合；
- P(x)表示信息x发生的概率，例如“骰子落地显示5点”这个信息的发生概率是1/6；
- h(x)表示信息x所携带的信息量；

##信息量

以上的信息熵公式中的信息量h(x)如何定义？

首先这是纯粹由人来设定的含义，应该方便于表达和计算。克劳德香农主要考虑到信息量应该具有以下特征：

- 发生概率越高的事件，信息量越小，信息所携带的信息量和概率成反比，即h(x)=m*1/P(x)。“硬币正面朝上”这个信息要比“骰子出现5点”所蕴含的信息少，对于“太阳是从东方升起的”这样完全确定的废话包含的信息应该是0.
- 信息量不可能是负的，不能因为你得到了一个新的信息，反而知道的更少了。
- 信息量应该是可以累加的，如果两个信息互相独立，比如“A:投出点数不是4”，“B:投出点数不是3”，这两个信息的信息合并成为一个信息后，如“C:投出的点数既不是4也不是3”，那么它的信息量应该等于前两者之和，即：h(C)=h(A)+h(B)。

我们知道，多个事件叠加的结果需要概率相乘，比如两个骰子，“A：其中一个投出6点”，“B：另一个投出5点”，那么叠加后“C：一个投出6点，另一个投出5点”，对于概率应该是P(C)=P(A)·P(B)，这里C事件出现的概率是1/6乘1/6等于1/36。

矛盾出现了，h(x)和1/P(x)成正比，但是又要满足$h(x_1,x_2)=h(x_1)+h(x_2)$和$P(x_1,x_2)=P(x_1)·P(x_2)$，这可能吗？

可以的，香农经过数学推理之后得到结论，信息量必须是可能性P的倒数的对数：


$$h(x)=\log{\frac {1}{P(x)}}$$

这里的对数log的底数可以是10或自然对数e或者任意数字，但在香农的信息论中都使用2。

**那么对于“硬币正面向上”这个信息，它的信息量就是$log_22=1$，这个也是香农设定的信息量单位，也叫香农单位，其实也对应了1比特。**

而对于四种平均随机可能的情况，每一种的信息量就是$log_24=2$，对应2比特的信息量。

骰子的每种情况的信息量是$log_26=2.585$，可以是小数。

这个信息量公式的另一种表达方式是改为：

$$h(x)=-\log P(x)$$

所以整体信息熵的计算公式就是：

$$H(X)=\sum _{x \in U }P(x)\log{\frac {1}{P(x)}}$$

或写作：
$$H(X)=-\sum _{x \in U }P(x)\log P(x)$$

按照这个公式计算扔硬币系统的信息熵是$1/2\times 1 +1/2\times 1 =1$，而四种可能性的随机系统的信息熵是$(2\times \frac{1}{4})\times 4=2$，骰子系统的信息熵是$(2.585\times \frac{1}{6})\times 6=2.585$。

很明显，系统的信息熵和单条信息量是相等的。但请注意，这里存在一个前提，那就是：**此条信息必须能够让系统变得完全确定**。对于“骰子投出的点数大于3”这样的信息就不可以简单的用这样的算法来计算。

**如果一条信息能够消除系统所有的不确定性，那么它所蕴含的信息量与整个系统的信息熵一样多。**

硬币和骰子和四种可能的例子几乎都是所有事件（每条信息）的发生概率相等的情况，对于更复杂的情况我们将在后面的文章中继续讨论。

---
欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)

---
###每个人的智能新时代
如果您发现文章错误，请不吝留言指正；
如果您觉得有用，请点喜欢；
如果您觉得很有用，欢迎转载~
---
END