欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)
[【汇总】2019年4月专题](https://www.jianshu.com/p/e1afed853866)

---

什么是条件熵？什么是信息增益？它的作用是什么？

![](imgs/4324074-339a19757628cac9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



##条件熵Conditional entropy

如前面文章所说的，[人工智能通识-科普-什么是熵](https://www.jianshu.com/p/cb7a17d81aac)，**熵是指系统的不确定性、随机性，这种性质是以系统输出的数据结果进行表现的，如硬币和骰子的统计数据，所以也可以看做数据的不确定性、随机性。**

系统信息熵的计算公式是：

$$H(X)=-\sum _{x \in U }P(x)\log P(x)$$

但当我们获得更多消息的时候，系统的不确定性就会减少。


![](imgs/4324074-85a6ab0d925a0ce1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

比如说这个问题，明天会下雨吗？假设我们有历史上每天是否下雨的1000条记录，其中100天下雨，900天不下，那么我们这个系统的信息熵可以计算：

$$
\begin{align}
H(是否下雨)&=-(\frac{1}{10}\times \log\frac{1}{10}+\frac{9}{10}\times \log\frac{9}{10})\\
&=-(0.1\times-3.3219-0.9\times 0.152)\\
&=0.3219+1.368\\
&=1.6899
\end{align}
$$

但是，假如我们知道明天是否阴天。因为阴天经常会导致下雨，所以那么明天下雨的确定性就会上升，不确定性就会下降，熵就会减少。

$$H(是否下雨|已知是否阴天)$$

这个就是**条件熵，即在某个条件下，数据变化的不确定性**。

##条件熵的计算

![](imgs/4324074-3a02c65ece0d2b18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


仍然以下雨为例，比如上面1000天的数据中，200天是阴天的，800天使不阴天的（晴天），其中阴天情况下90天下雨，不阴天情况也有10天下雨（太阳雨 ฅ´ω`ฅ）。

那么已知阴天情况下，共200天，90天下雨，110天不下雨，是否下雨的条件熵是：

$$
\begin{align}
H(是否下雨|是否阴天=是)&=H(Y|X=Yes)\\
&=-(\frac{90}{200}\times \log\frac{90}{200}+\frac{110}{200}\times \log\frac{110}{200})\\
&=0.5184+0.4744\\
&=0.993\\
\end{align}
$$

这个条件熵接近1，就是阴天且下雨的概率接近一半一半。
>熵是1就是正反面一半一半，信息量是1比特就是可以消除50%的不确定性。

同样的，已知明天不阴天（晴天）的情况下，共800天，790天不下雨，10天下雨，这个条件熵是：


$$
\begin{align}
H(是否下雨|是否阴天=否)&=H(Y|X=No)\\
&=-(\frac{790}{800}\times \log\frac{790}{800}+\frac{10}{800}\times \log\frac{10}{800})\\
&=0.0179+0.079\\
&=0.0969\\
\end{align}
$$

这个条件熵很低，确定性很高，晴天当然可以几乎确定是不下雨的。

>如果熵为0，则说明绝对的确定。

但上面只是分开计算了阴天=是和阴天=否的情况，我们还要把它们按照概率比例相加一起才算是整个阴天与否条件下是否下雨的条件熵：

$$
\begin{align}
H(是否下雨|是否阴天)&=H(Y|X)\\
&=P(X=Yes)\times H(Y|X=Yes)+P(X=No)\times H(Y|X=No)\\
&=\frac{200}{1000}\times 1.6899+\frac{800}{1000}\times 0.969\\
&=0.1986+0.7752\\
&=0.9738 \\
\end{align}
$$

近乎于1，也就是说，如果我们知道明天是否阴天，那么是否下雨也就基本确定了一半，这和200个阴天有90天下雨的感性认知基本一致。

总结上面我们计算方法，整体条件熵等于条件每个可能值的条件熵之概率加权和：

$$
\begin{align}
H(Y|X)=\sum_{x\in X}P(x)\log\frac{1}{P(x)}
\end{align}
$$

而每个可能值的条件熵，计算方法基本上和信息熵公式一致：

$$
\begin{align}
H(Y|X=A)=\sum_{y\in Y}P(Y|X=A)\log\frac{1}{P(Y|X=A)}
\end{align}
$$


##信息增益Information Gain

信息增益是指某个信息条件下，系统整体的熵减少了多少，也就是整体信息熵减去条件信息熵的结果。

$$InformationGain(Y|X)=H(Y)-H(Y|X)$$

信息增益有什么用？

![](imgs/4324074-c62d2a68dff88e18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们知道，世界上某个结果的出现往往是很多原因作用的结果，比如下雨这个事情就可能受到气温、气压、温度、湿度等等多种原因的影响。
但是，各种因素中哪一个因素对下雨影响最大？哪一些影响比较小？


如果我们也有1000天的气温、气压、温度、湿度数据，我们就可以计算出它们分别的条件熵，因为条件熵越大，那么就对结果的影响越大。

以上面的例子，是否阴天这个条件可以让是否下雨的不确定性下降1.6899-0.9738=0.7161，这个作用是非常明显的，相当于问你“明天有多大概率下雨？”和“明天如果阴天的话有多大概率下雨？”的差别。


>下一篇我们将用更完整一些的案例来深化信息增益的计算方法和应用价值。

---
欢迎关注我的专栏( つ•̀ω•́)つ[【人工智能通识】](https://www.jianshu.com/c/e9a7b7b7024d)

---
###每个人的智能新时代
如果您发现文章错误，请不吝留言指正；
如果您觉得有用，请点喜欢；
如果您觉得很有用，欢迎转载~
---
END