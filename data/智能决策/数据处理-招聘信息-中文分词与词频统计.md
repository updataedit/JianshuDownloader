[智能决策上手系列教程索引](https://www.jianshu.com/p/0d2e46e69f58)
---
通过前面的几篇文章，相信大家都尝试抓取了一些网站上招聘信息的数据，并存储到自己的文件里面了，可能是一堆`.json`或`.csv`文件。
如果你还没有抓到数据，请看这个[网络数据抓取-拉勾网职位列表和详情-requests案例](https://www.jianshu.com/p/c240daad86a2)

### 为什么要分词？
分词也叫切词，cut。
以我们抓取的招聘职位的例子，我们需要了解掌握哪些技术才能找到一个人工智能方面的工作，简单的办法就是看数百个职位招聘的详情里面哪些词出现的次数最多，比如`Python`这个词肯定出现频率很高，`机器学习`和`深度学习`,`tensorflow`之类的单词出现的也应该比较高，当然还有很多其他的词，比如`pytorch`重要吗？`coffee`重要吗？`神经网络`重要吗？

**不能靠猜！**
必须用统计数据说话，简单的思路就是看看哪些词出现次数最多。

可是我们并不熟悉人工智能技术啊，也就不知道要统计哪些词的频率才合适。
小白就不能做研究作分析了吗？当然可以。

我们可以用一个大词典来作参照，让计算机把数百条招聘详情的文字段落都拆成一个词一个词的，然后再把出现频率最多的告诉我们就可以了~

比如把`熟悉使用 Python 进行数据处理和分析，熟练使用Numpy, Pandas, scikit-learn等科学计算库`切成`熟悉/使用/Python/进行/数据处理/和/分析/熟悉/使用/Numpy/Pandas/scikit-learn/等/科学/计算库`。
这就得到了`['熟悉','使用','Python','进行','数据处理','和','分析','Numpy','Pandas','scikit-learn','等','科学','计算库']`这些单词，注意`熟悉`和`使用`两个词不重复记录，我们把数百条招聘信息放在一起，切割，汇总，就得到了一个包含很多很多词的列表，这里的每个词至少被提及了一次。

### 词袋模型BOW

分词的目的是为了两个事情：
1. 得到一个所有用到的词的列表，比如`['我','你','爱','学习']`。
1. 可以用这个列表来表示任何一个文字语句段落，比如用上面这个词表，`我爱你，也爱学习`就可以表示为`[2,1,2,1]`,`我爱你`就可以表示成`[0,1,1,1]`，这里的每个数字表示这个词在原文里面出现的次数，出现次数越多，频率越高。


![image.png](imgs/4324074-9149bab04a4432e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这就好像把一句话切成很多段，然后胡乱的扔进一个袋子里，虽然顺序已经丢失了，但是每个袋子里某个词的数量仍然可以统计，这就是词袋模型Bag-Of-Words，BOW。
![image.png](imgs/4324074-73d70ee9dd7b100e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 向量空间模型 VSM

我们知道平面向量坐标是`[x,y]`，比如`[10,10]`就是右上方45度的一个点的位置；换到三维空间`[x,y,z]`立体向量我们也不怕。
![image.png](imgs/4324074-c851d84d4f1e591f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




但是上面那个`我不爱学习``[2,1,2,1]`怎么样呢？四个数字当然是四维向量了。以此类推，就算是128维我们也不怕了。
虽然我们人类的大脑想象不出四维或更多维的空间，但数字运算我们可以参照二三维的算法依葫芦画瓢就可以。

把一个句子或一段文字切割成一排单词，进而用一个数字列表来表示，一个句子或一段文字看上去像是一个N维向量，这就是向量空间模型表示法，VSM，Vector Space Model。它是词袋BOW模型的延续。

### 安装结巴jieba

结巴就是口吃，口吃的人说话经常一个词一个词的卡住，恰好有国内开发者就为大家编写了一个叫做jieba的分词功能模块，它很强大，我们先安装它。

打开命令行或终端工具，输入：
`pip3 install jieba`
稍等一会就能自动完成安装。如果有提示要升级upgrade你的pip，就可以参照提示输入命令升级。

**[结巴jieba官方Github项目网址，有详细教程](https://github.com/fxsjy/jieba)**

Notebook新建一个Python 3文件，输入下面代码测试：
```
import jieba
fc=jieba.cut('我爱你，我爱学习')
print('/'.join(fc))
```
输出结果`我爱你/，/我/爱/学习`和我们想的不太一样，管他呢，反正我们安装成功了。

### 统计单个招聘信息的词频

先读取一个职位信息数据文件，然后提取到details细节文字，转小写lower，再替换掉里面的空格冗余，然后用jieba切词cut。

>[如果您还没有抓取，请从这里直接下载100个json搁置职位文件](https://pan.baidu.com/s/1_vfiIdIPGzAyBuRx4__BsA)  密码:tfdv

以下是代码，注意根据自己情况替换文件路径名称：
```
#cell-1
import jieba

def readDetail(fileName):
    with open(fileName, 'r') as f:
        job = json.load(f)
        details = job['details'].lower()
        details = details.replace(' ', '').replace('\xa0', '')
        return details


text = readDetail('./data/lagou_ai/jobs1000/362654.json')
fc = jieba.cut(text)
print('/'.join(fc))
```
这里注意到数据文件中包含的空格有两种，只用`replace(' ','')`去不掉。可以先用`print(repr(details))`方法打印出来，就会看到还有`\xa0`这种空格了。可以参考下面一小段代码理解,其中re是正则表达式，如果没接触过可以先忽略它：
```
import re
s='AA \xa0BB'
print(s)
print(repr(s))
print(s.replace(' ',''))
print(s.replace(' ','').replace('\xa0',u''))
print(re.sub('\s+','',s))
```
输出结果：
AA  BB
'AA \xa0BB'
AA BB
AABB
AABB

分词cell-1代码运行得到的结果类似：
`职位/描述/：/*/爱奇艺/福利/：/14/薪/、/12/天年/假/，/超一流/的/行政/团队/，/日常/活动/多多/,/没有/996/，/周一/到/周五/及/周末/自愿/加班/都/有/丰厚/的/加班/补贴/*/团队/介绍/：/*/我们/是/一个/信仰/技术/的/团队/，/在/这里/，/我们/用/技术/说话/，/相信/技术/和/人工智能/能/改变/生活/。/目前/团队/...`

### 关键词提取

哪些词是最重要的？不仅仅要考虑出现频率，比如很多语气词助词出现频率高但没有意义。
jieba提供了简单有效的分析提取工具analyse，有两种方式：
* `extract_tags`提取标记，使用的TF/IDF(term frequency术语频率/Inverse document frequency反向文档频率)算法获得单词的权重。语法是` jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`，其中topK是列出最重要的20个。
* `textrank`是另一种算法，具体参照jieba官方说明，直接用就可以，`jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))`

我们把两个算法都测试一下：
```
#cell-2
import jieba.analyse

for word, weight in jieba.analyse.extract_tags(text, withWeight=True):
    print('%s %s' % (word, weight))
    
print('-'*20)

for word, weight in jieba.analyse.textrank(text, withWeight=True):
    print('%s %s' % (word, weight))
```
输出的结果是：
爱奇艺 0.2134779911232143
nlp 0.2134779911232143
团队 0.21031455098690477
负责 0.17199107835744049
文本 0.15972947489946426
...
`--------------------`
负责 1.0
分析 0.8929318143543986
团队 0.8835148710340713
技术 0.8233113604163651
搜索 0.6789881049317539
...
看上去第一个比较好些，至少提取到了nlp这个关键技术点。

### 从批量数据中提取关键词
我们先读取100个招聘职位数据文件，把details都连接到一起然后输出关键词。
为了避免销售、运营等非技术职位的干扰，我们只使用包含python或tensorflow的职位。
为了避免隐藏文件导致错误，判断文件名必须包含`.json`才执行。
下面是完整代码：
```
#cell-1
import jieba

def readDetail(fileName):
    with open(fileName, 'r') as f:
        job = json.load(f)
        details = job['details'].lower()
        details = details.replace(' ', '').replace('\xa0', '')
        return details

#cell-2
import os

text = ''
files = os.listdir('./data/lagou_ai/jobs1000/')
jobCount = 0
for n in range(0, 1000):
    if not files[n].find('.json')==-1:
        details = readDetail('./data/lagou_ai/jobs1000/' + files[n])
        if details.find('python') != -1 or details.find('tensorflow') != -1:
            jobCount += 1
            text += details
print('>>Got jobs:', jobCount)

#cell-3
import jieba.analyse

for word, weight in jieba.analyse.extract_tags(text, topK=50, withWeight=True):
    print('%s %s' % (word, weight))
```
运行得到350个有效职位，最后100关键词列表(下面去除了一些无关的单词，但仍然比较长，仅供参考)：
算法 0.16280807037359346
学习 0.12521739675971214
python 0.09640941534596774
人工智能 0.09249022510194692
经验 0.0903619452278592
机器 0.076475520044677
开发 0.06282155542603188
能力 0.05860484859142685
技术 0.05634099247975362
数据 0.051716054158354285
java 0.049343401555022856
c++ 0.0485842723003302
数据挖掘 0.044729592655659976
模型 0.04294333614698163
应用 0.03801291673277877
团队 0.03739400195347558
优化 0.037294334549497286
计算机 0.03615275785604267
分析 0.034857880483786303
自然语言 0.034456246568351535
编程 0.031659378238450595
数据分析 0.031202021961397427
平台 0.03053176386762446
研发 0.03041435802634324
设计 0.03018368404402845
任职 0.030103547088539916
数学 0.02881462488389637
系统 0.02866245418612649
框架 0.028054525207817503
业务 0.027955582209485647
建模 0.027914137122792313
linux 0.027834739338730843
nlp 0.027834739338730843
专业 0.02687323086027644
tensorflow 0.026316480829345526
用户 0.024837901203459484
编程语言 0.023921795978702056
研究 0.023795093950279404
spark 0.023786049980369994
识别 0.02346950050499534
产品 0.022570152356689105
项目 0.02191093140143595
文本 0.020637302404248794
硕士 0.020465162362061213
语言 0.019699604895756923
hadoop 0.01948431753711159
海量 0.018637449359492422
场景 0.01840435808542905
沟通 0.018238597080262466
caffe 0.018219102112623824
视觉 0.018215512260423124
本科 0.017933459663175007
挖掘 0.01787062300279972
架构 0.017215790470250614
语音 0.016999498912654516
ai 0.016700843603238508
基础 0.016534794244883584
智能 0.016519711579636144
语义 0.01593578503036449
问题 0.015268805992307172
神经网络 0.015112282160282786
理解 0.015094887961947762
计算机相关 0.014929542008955634
互联网 0.014923956114526925
数据结构 0.014867715551689104
图像 0.014776146846297944
优秀 0.014214794317419989

### 总结

这篇文章非常的基础，我们初步了解了利用jieba切词和关键词提取的方法，虽然得到一些数据统计结论，但其实还有很多问题，只是一个开端，后面的文章我们再慢慢深入探索。




---
[智能决策上手系列教程索引](https://www.jianshu.com/p/0d2e46e69f58)
---
###每个人的智能决策新时代
如果您发现文章错误，请不吝留言指正；
如果您觉得有用，请点喜欢；
如果您觉得很有用，欢迎转载~
---
END